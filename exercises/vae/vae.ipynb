{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyd6EGDKUkYI",
        "outputId": "c88a455f-6592-4fdc-a24c-1b12a715f9e9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FyIVp78UuM9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import jax\n",
        "import numpy as np\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "try:\n",
        "  import flax\n",
        "except ModuleNotFoundError:\n",
        "  !pip install --quiet flax==0.6.4\n",
        "  import flax\n",
        "\n",
        "try:\n",
        "  import optax\n",
        "except ModuleNotFoundError:\n",
        "  !pip install --quiet optax\n",
        "  import optax\n",
        "\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "from tqdm import trange\n",
        "from functools import partial\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeMNHRUK5SOl"
      },
      "source": [
        "# Variational Auto-Encoder (VAE)\n",
        "\n",
        "Remind that VAE minimizes the following variational bound\n",
        "$$\\text{KL}(p_{\\text{data}}(x),p(x|\\theta)) \\leq \\text{KL}(p_{\\text{data}}(x)q(z|x;\\eta),p(x|z;\\theta)p(z|\\theta))$$\n",
        "i.e. \n",
        "$$\\theta^*,\\eta^* = \\argmin_{\\theta,\\eta} \\text{KL}(p_{\\text{data}}(x)q(z|x;\\eta),p(x|z;\\theta)p(z|\\theta)).$$\n",
        "Eliminating the terms that don't depend on $\\theta,\\eta$ and rearranging them, we get\n",
        "$$\\text{Loss}(\\eta,\\theta) = -\\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{q(z|x;\\eta)} \\log p(x|z;\\theta) + \\mathbb{E}_{p_{\\text{data}}(x)}\\text{KL}(q(z|x;\\eta), p(z|\\theta)).$$\n",
        "In particular, here we consider the following parameterization.\n",
        "The approximate posterior is parameterised as a normal distribution\n",
        "$$q(z|x;\\eta) = \\mathcal{N}(z|\\mu(x;\\eta),\\sigma(x;\\eta)^2),$$\n",
        "which parameters $\\mu(x;\\eta),\\sigma(x;\\eta)$ are defined by a neural network called \"encoder\".\n",
        "The join likelihood is defined by another normal distribution\n",
        "$$p(x|z;\\theta) = \\mathcal{N}(x|\\mu(z;\\theta),\\sigma(z;\\theta)^2),$$\n",
        "where $\\mu(z;\\theta),\\sigma(z;\\theta)$ is defined by the \"decoder\", and the marginal distribution of latent variables is the standard normal gaussian, i.e.\n",
        "$$p(z|\\theta) = \\mathcal{N}(z|0,1).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShYUPPpY-wm6"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple, Any\n",
        "\n",
        "def sample_data(key, bs):\n",
        "  keys = random.split(key, 3)\n",
        "  label = random.randint(keys[0], minval=0, maxval=2, shape=(bs, 2))\n",
        "  x_1 = 3*(label.astype(jnp.float32)-0.5)\n",
        "  x_1 += 4e-1*random.normal(keys[1], shape=(bs,2))\n",
        "  return x_1, label\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "key = random.PRNGKey(seed)\n",
        "bs = 512\n",
        "\n",
        "key, ikey = random.split(key)\n",
        "data, labels = sample_data(ikey, 1024)\n",
        "plt.scatter(data[:,0], data[:,1], alpha=0.3, label='data')\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ediByodYRT0y"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "For parameterizing both encoder and decoder, we are going to use an MLP that has two outputs: one to parameterize the mean and another to parameterize the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N47QZckXbes"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  num_hid : int\n",
        "  num_out : int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    h = x\n",
        "    h = nn.Dense(features=self.num_hid)(h)\n",
        "    h = nn.relu(h)\n",
        "    h = nn.Dense(features=self.num_hid)(h)\n",
        "    h = nn.swish(h)\n",
        "    h = nn.Dense(features=self.num_hid)(h)\n",
        "    h = nn.swish(h)\n",
        "    h = nn.Dense(features=2*self.num_out)(h)\n",
        "    return h[:,:self.num_out], h[:,self.num_out:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-KMR9YEJKtc",
        "outputId": "f3592040-1832-4769-e0df-40d30e7ee753"
      },
      "outputs": [],
      "source": [
        "latent_dim = 2\n",
        "encoder = MLP(num_hid=512, num_out=latent_dim)\n",
        "decoder = MLP(num_hid=512, num_out=data.shape[1])\n",
        "print(encoder, decoder)\n",
        "key, *init_key = random.split(key, 3)\n",
        "state_enc = train_state.TrainState.create(apply_fn=encoder.apply,\n",
        "                                          params=encoder.init(init_key[0], data),\n",
        "                                          tx=optax.adam(learning_rate=2e-4))\n",
        "state_dec = train_state.TrainState.create(apply_fn=decoder.apply,\n",
        "                                          params=decoder.init(init_key[1], jnp.ones((data.shape[0], latent_dim))),\n",
        "                                          tx=optax.adam(learning_rate=2e-4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jy0rZtBRPSi"
      },
      "source": [
        "## Loss Function (5 points)\n",
        "In general, the loss looks like this\n",
        "$$\\text{Loss}(\\eta,\\theta) = -\\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{q(z|x;\\eta)} \\log p(x|z;\\theta) + \\mathbb{E}_{p_{\\text{data}}(x)}\\text{KL}(q(z|x;\\eta), p(z|\\theta))$$\n",
        "- Derive the loss for the encoder, decoder, prior parameterized as Gaussians (as described above). Note that you have to find the best estimation, i.e. reduce the objective to the minimum number of Monte Carlo estimates possible. \n",
        "- Implement the objective in the function below.\n",
        "- Train the model with the derived objective. You can generate new batch at each step using the 'sample_data' function. Does the loss curve look meaningful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcFL4cmx_oaL"
      },
      "outputs": [],
      "source": [
        "def vae_objective(states, key, params, bs):\n",
        "  # YOUR CODE HERE\n",
        "  print(loss.shape, 'final.shape', flush=True)\n",
        "  return loss.mean()\n",
        "\n",
        "@partial(jax.jit, static_argnums=2)\n",
        "def train_step(state_enc, state_dec, bs, key):\n",
        "  grad_fn = jax.value_and_grad(vae_objective, argnums=2)\n",
        "  loss, grads = grad_fn((state_enc, state_dec), key, (state_enc.params, state_dec.params), bs)\n",
        "  state_enc = state_enc.apply_gradients(grads=grads[0])\n",
        "  state_dec = state_dec.apply_gradients(grads=grads[1])\n",
        "  return state_enc, state_dec, loss\n",
        "\n",
        "key, loc_key = random.split(key)\n",
        "state_enc, state_dec, loss = train_step(state_enc, state_dec, bs, loc_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXhQos3BRMXv"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sAcDM4XFT6X",
        "outputId": "850b3939-820f-4cf4-fdf7-78981c718c01"
      },
      "outputs": [],
      "source": [
        "num_iterations = 10_000\n",
        "\n",
        "loss_plot = np.zeros(num_iterations)\n",
        "for iter in trange(num_iterations):\n",
        "  key, loc_key = random.split(key)\n",
        "  state_enc, state_dec, loss = train_step(state_enc, state_dec, bs, loc_key)\n",
        "  loss_plot[iter] = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "ELQg1NUbHJec",
        "outputId": "c3bf51cc-8bef-461b-c973-32353472015a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aD4SO2IVMkg"
      },
      "source": [
        "## Visualization of the Trained Model (5 points for both cells)\n",
        "For the trained VAE\n",
        "- Visualize generated samples using $p(x|z;\\theta)p(z|\\theta)$ and compare them to the samples from the data. Do they match?\n",
        "- Visualize the latent distribution by sampling from $q(z|x;\\eta)p_{\\text{data}}(x)$ and compare them to the samples from $p(z|\\theta)$. Do they match?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, *loc_key = random.split(key,5)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='data')\n",
        "plt.scatter(# YOUR CODE HERE, \n",
        "            alpha=0.3, label='generations')\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "plt.subplot(122)\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='prior')\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='encoded data')\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize how the latent space is divided between the modes. There is more than one way to visualize this. For instance, use the variable labels produced by 'sample_data', encode the data and color the samples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "for # YOUR CODE HERE\n",
        "  plt.scatter(# YOUR CODE HERE,\n",
        "              alpha=0.3)\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(122)\n",
        "for # YOUR CODE HERE\n",
        "  plt.scatter(# YOUR CODE HERE, \n",
        "              alpha=0.3)\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the trained model (5 points)\n",
        "\n",
        "Let's evaluate the model by estimating $\\mathbb{E}_{p_{\\text{data}}(x)}\\log p(x|\\theta)$. Namely, consider three different estimates:\n",
        "1. Naive marginal likelihood estimation \n",
        "$$\\mathbb{E}_{p_{\\text{data}}(x)}\\log p(x|\\theta) = \\mathbb{E}_{p_{\\text{data}}(x)}\\log \\int dz\\;p(x|z;\\theta)p(z|\\theta)$$\n",
        "2. VAE lower bound\n",
        "$$\\mathbb{E}_{p_{\\text{data}}(x)}\\log p(x|\\theta) \\geq \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{q(z|x;\\eta)}\\log \\frac{p(x|z;\\theta)p(z|\\theta)}{q(z|x;\\eta)} \\approx \\mathbb{E}_{p_{\\text{data}}(x)} \\frac{1}{K}\\sum_{k=1}^K\\log \\frac{p(x|z_k;\\theta)p(z_k|\\theta)}{q(z_k|x;\\eta)}, \\;\\; z_k \\sim q(z|x;\\eta)$$\n",
        "3. IWAE lower bound\n",
        "$$\\mathbb{E}_{p_{\\text{data}}(x)}\\log p(x|\\theta) \\geq \\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{z_1,\\ldots, z_k \\sim q(z|x;\\eta)}\\log \\frac{1}{K}\\sum_{k=1}^K\\frac{p(x|z_k;\\theta)p(z_k|\\theta)}{q(z_k|x;\\eta)}$$\n",
        "\n",
        "Implement the functions below and compare them by plotting the log-likelihood estimates with the number of samples and estimating the density on the state-space. Which estimate is better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def marginal_likelihood(key, x, n_samples, state_enc, state_dec):\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "def vae_bound(key, x, n_samples, state_enc, state_dec):\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "def iwae_bound(key, x, n_samples, state_enc, state_dec):\n",
        "  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, *ikey = random.split(key, 3)\n",
        "print('vae bound:', vae_bound(ikey[1], data, 1000, state_enc, state_dec).mean())\n",
        "print('iwae bound:', iwae_bound(ikey[1], data, 1000, state_enc, state_dec).mean())\n",
        "\n",
        "n_samples = [1,10,100,1000]\n",
        "ll_iwae = []\n",
        "ll_vae = []\n",
        "for n in n_samples:\n",
        "  ll_vae.append(vae_bound(ikey[1], data, n, state_enc, state_dec).mean())\n",
        "  ll_iwae.append(iwae_bound(ikey[1], data, n, state_enc, state_dec).mean())\n",
        "  \n",
        "plt.plot(n_samples, ll_vae)\n",
        "plt.plot(n_samples, ll_iwae)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('number of samples')\n",
        "plt.ylabel('likelihood estimate')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, *ikey = random.split(key, 4)\n",
        "x,y = jnp.linspace(-4,4,50), jnp.linspace(-4,4,50)\n",
        "x,y = jnp.meshgrid(x,y)\n",
        "grid = jnp.stack([x, y]).transpose((1,2,0)).reshape((-1,2))\n",
        "\n",
        "for n_samples in [1,10,100,1000]:\n",
        "  plt.figure(figsize=(16,5))\n",
        "  \n",
        "  ll = marginal_likelihood(ikey[0], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(131)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "  \n",
        "  ll = vae_bound(ikey[1], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(132)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "\n",
        "  ll = iwae_bound(ikey[2], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(133)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importance Weighted Autoencoder (IWAE)\n",
        "\n",
        "Why don't we try learning the entire model using IWAE?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dim = 2\n",
        "encoder = MLP(num_hid=512, num_out=latent_dim)\n",
        "decoder = MLP(num_hid=512, num_out=data.shape[1])\n",
        "print(encoder, decoder)\n",
        "key, *init_key = random.split(key, 3)\n",
        "state_enc = train_state.TrainState.create(apply_fn=encoder.apply,\n",
        "                                          params=encoder.init(init_key[0], data),\n",
        "                                          tx=optax.adam(learning_rate=2e-4))\n",
        "state_dec = train_state.TrainState.create(apply_fn=decoder.apply,\n",
        "                                          params=decoder.init(init_key[1], jnp.ones((data.shape[0], latent_dim))),\n",
        "                                          tx=optax.adam(learning_rate=2e-4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IWAE objective (5 points)\n",
        "\n",
        "- Implement the IWAE objective, i.e. \n",
        "$$\\theta^*,\\eta^* = \\argmin_{\\theta,\\eta}-\\mathbb{E}_{p_{\\text{data}}(x)}\\mathbb{E}_{z_1,\\ldots, z_k \\sim q(z|x;\\eta)}\\log \\frac{1}{K}\\sum_{k=1}^K\\frac{p(x|z_k;\\theta)p(z_k|\\theta)}{q(z_k|x;\\eta)}$$\n",
        "- Train the model using the IWAE objective and $K=5$ at every iteration. How does the loss function behave?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iwae_objective(states, key, params, bs):\n",
        "  # YOUR CODE HERE\n",
        "  print(loss.shape, 'final.shape', flush=True)\n",
        "  return loss.mean()\n",
        "\n",
        "@partial(jax.jit, static_argnums=2)\n",
        "def train_step(state_enc, state_dec, bs, key):\n",
        "  grad_fn = jax.value_and_grad(iwae_objective, argnums=2)\n",
        "  loss, grads = grad_fn((state_enc, state_dec), key, (state_enc.params, state_dec.params), bs)\n",
        "  state_enc = state_enc.apply_gradients(grads=grads[0])\n",
        "  state_dec = state_dec.apply_gradients(grads=grads[1])\n",
        "  return state_enc, state_dec, loss\n",
        "\n",
        "key, loc_key = random.split(key)\n",
        "state_enc, state_dec, loss = train_step(state_enc, state_dec, bs, loc_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_iterations = 10_000\n",
        "\n",
        "loss_plot = np.zeros(num_iterations)\n",
        "for iter in trange(num_iterations):\n",
        "  key, loc_key = random.split(key)\n",
        "  state_enc, state_dec, loss = train_step(state_enc, state_dec, bs, loc_key)\n",
        "  loss_plot[iter] = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of IWAE (3 points)\n",
        "\n",
        "- Reproduce the visualizations that you did for VAE for the IWAE.\n",
        "- Reproduce the plots for the density and log-likelihood estimation.\n",
        "- How can we compare IWAE and VAE? Which model is better? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='data')\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='generations')\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "plt.subplot(122)\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='prior')\n",
        "plt.scatter(# YOUR CODE HERE,\n",
        "            alpha=0.3, label='encoded data')\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "for # YOUR CODE HERE\n",
        "  plt.scatter(# YOUR CODE HERE, \n",
        "              alpha=0.3)\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(122)\n",
        "for # YOUR CODE HERE\n",
        "  plt.scatter(# YOUR CODE HERE, \n",
        "              alpha=0.3)\n",
        "plt.xlim(-4,4)\n",
        "plt.ylim(-4,4)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, *ikey = random.split(key, 4)\n",
        "x,y = jnp.linspace(-4,4,50), jnp.linspace(-4,4,50)\n",
        "x,y = jnp.meshgrid(x,y)\n",
        "grid = jnp.stack([x, y]).transpose((1,2,0)).reshape((-1,2))\n",
        "\n",
        "for n_samples in [1,10,100,1000]:\n",
        "  plt.figure(figsize=(16,5))\n",
        "  \n",
        "  ll = marginal_likelihood(ikey[0], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(131)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "  \n",
        "  ll = vae_bound(ikey[1], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(132)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "\n",
        "  ll = iwae_bound(ikey[2], grid, n_samples, state_enc, state_dec)\n",
        "  plt.subplot(133)\n",
        "  plt.contourf(x,y,jnp.exp(ll).reshape(x.shape), levels=50)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, *ikey = random.split(key, 3)\n",
        "print('vae bound:', vae_bound(ikey[1], data, 1000, state_enc, state_dec).mean())\n",
        "print('iwae bound:', iwae_bound(ikey[1], data, 1000, state_enc, state_dec).mean())\n",
        "\n",
        "n_samples = [1,10,100,1000]\n",
        "ll_iwae_trained = []\n",
        "for n in n_samples:\n",
        "  ll_iwae_trained.append(iwae_bound(ikey[1], data, n, state_enc, state_dec).mean())\n",
        "  \n",
        "plt.plot(n_samples, ll_iwae)\n",
        "plt.plot(n_samples, ll_iwae_trained)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('number of samples')\n",
        "plt.ylabel('likelihood estimate')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps? (Open-ended questions, 2 points)\n",
        "- What are the next steps for exploring VAE and IWAE?\n",
        "- Can you come up with a 2d dataset that highlights their differences?\n",
        "- What happens if we change the dimensionality of the latent space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.11 ('jax-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "9b5010b7c43642655f9a773276a2f783938f8332af16a6f04f16ff491d6a6741"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
